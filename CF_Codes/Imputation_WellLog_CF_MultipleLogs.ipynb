{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50JA4KkTSZF3"
   },
   "source": [
    "The following file demonstrates the process and code used in our research:\n",
    "\n",
    "**Imputation of missing values in well log data using k-nearest neighbor collaborative filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Moc22WGLYGqB"
   },
   "source": [
    "The following python libraries are utilized in our research:\n",
    "\n",
    "*   NumPy\n",
    "*   Pandas\n",
    "*   Matplotlib\n",
    "*   Scikit-Learn\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbjolIGLV6aQ"
   },
   "source": [
    "Important to Note:\n",
    "* Run each cell sequentially unless stated otherwise\n",
    "* Some cells may take long time to run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1767,
     "status": "ok",
     "timestamp": 1707877019699,
     "user": {
      "displayName": "­김민준 / 학생 / 에너지자원공학과",
      "userId": "17466341483235884378"
     },
     "user_tz": -540
    },
    "id": "1yXXE3NjXED8"
   },
   "outputs": [],
   "source": [
    "# Import all needed python libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "# To ignore warnings (not needed)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha0gGvCYW8AR"
   },
   "source": [
    "# Import Well Log Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK-BY228a33j"
   },
   "source": [
    "Import Well Log Data Files: \"CSV_train.csv\", \"CSV_hidden_test.csv\" using Pandas Dataframe and make necessary adjusments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "error",
     "timestamp": 1707876289005,
     "user": {
      "displayName": "­김민준 / 학생 / 에너지자원공학과",
      "userId": "17466341483235884378"
     },
     "user_tz": -540
    },
    "id": "rwFHcqbdW-jZ",
    "outputId": "d2830420-0143-44cb-a797-89960246761a"
   },
   "outputs": [],
   "source": [
    "# Import DF and Combine data_set\n",
    "df_labeled_1 = pd.read_csv('CSV_train.csv', sep=';') # Change file address if nescessary\n",
    "df_labeled_2 = pd.read_csv('CSV_hidden_test.csv', sep=';') # Change file address if nescessary\n",
    "\n",
    "df_labeled = pd.concat([df_labeled_1, df_labeled_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "52l-urneW-9d"
   },
   "outputs": [],
   "source": [
    "# Add \"True Vertical Depth (TVD)\" column\n",
    "\n",
    "df_labeled['DEPTH_TVD'] = df_labeled['Z_LOC'] * -1\n",
    "column_to_move = df_labeled.pop('DEPTH_TVD')\n",
    "df_labeled.insert(2, 'DEPTH_TVD', column_to_move)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code makes adjustments of the lithology column for a more convenient analysis.\n",
    "\n",
    "Original data contains 12 lithology, but our research assumes only 11. \"Basement\" lithology has been removed as it is a very small portion in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Following cells adds a lithology column for a more convenient analysis\n",
    "\n",
    "lithology_numbers = {\n",
    "                 30000: 0,\n",
    "                 65030: 1,\n",
    "                 65000: 2,\n",
    "                 80000: 3,\n",
    "                 74000: 4,\n",
    "                 70000: 5,\n",
    "                 70032: 6,\n",
    "                 88000: 7,\n",
    "                 86000: 8,\n",
    "                 99000: 9,\n",
    "                 90000: 10,\n",
    "                 93000: 11\n",
    "                 }\n",
    "\n",
    "df_labeled=df_labeled.replace({\"FORCE_2020_LITHOFACIES_LITHOLOGY\": lithology_numbers})\n",
    "\n",
    "\n",
    "\n",
    "lithology_type = {\n",
    "                 0: 'Sandstone',\n",
    "                 1: 'ShalySand',\n",
    "                 2: 'Shale',\n",
    "                 3: 'Marl',\n",
    "                 4: 'Dolomite',\n",
    "                 5: 'Limestone',\n",
    "                 6: 'Chalk',\n",
    "                 7: 'Halite',\n",
    "                 8: 'Anhydrite',\n",
    "                 9: 'Tuff',\n",
    "                 10: 'Coal',\n",
    "                 11: 'Basement'\n",
    "                 }\n",
    "\n",
    "df_labeled['Lithology_Type'] = df_labeled['FORCE_2020_LITHOFACIES_LITHOLOGY'].map(lithology_type)\n",
    "\n",
    "# Drop basement lithology\n",
    "df_labeled  = df_labeled[df_labeled['Lithology_Type'] != 'Basement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null Cells in Dataframe with -9999\n",
    "\n",
    "df_labeled = df_labeled.fillna(-9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing (Test Data Fabrication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our research utilizes four log features: GR, RHOB, NPHI, DTC.\n",
    "\n",
    "Other log curves are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features used\n",
    "features = [\"WELL\", \"DEPTH_TVD\",'GR', 'RHOB', 'NPHI', 'DTC', 'Lithology_Type', 'FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "df_labeled_with_features_original = df_labeled[features]\n",
    "\n",
    "# Save file for later\n",
    "# This file represents the original data file used to validate our method.\n",
    "df_labeled_with_features_original.to_csv('df_labeled_with_features_original.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes represent the test data fabrication process for **when two logs are missing simultaneously**.\n",
    "\n",
    "Initially, RHOB, NPHI, DTC log curves representing 50 meters of data are intentionally removed.\n",
    "Process is repeated three times for each feature using different wells.\n",
    "\n",
    "The following log curves are also intentionally removed:\n",
    "1) For RHOB, DTC is also removed for the same depth intervals.\n",
    "2) For NPHI, RHOB is also removed for the same depth intervals.\n",
    "3) For DTC, NPHI is also removed for the same depth intervals.\n",
    "\n",
    "\n",
    "The removed datapoints are gathered to create the \"Test Data Matrix\". Remaining data is used to create the \"Neighbor Data Matrix\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Erase Parts of Each Log Curve Intentionally ##\n",
    "df_labeled_with_features_sampling = df_labeled_with_features_original.copy()\n",
    "df_labeled_with_features_sampling['WELL'] = df_labeled['WELL']\n",
    "df_labeled_with_features_sampling['DEPTH_TVD']=df_labeled['DEPTH_TVD']\n",
    "\n",
    "\n",
    "\n",
    "#For Well 35/11-10\n",
    "index_to_remove = df_labeled_with_features_sampling.index[(df_labeled_with_features_sampling['WELL'] == '35/11-10') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2600) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2650)].tolist()\n",
    "for index in index_to_remove:\n",
    "    for feature in ['RHOB', 'DTC']:\n",
    "        df_labeled_with_features_sampling.loc[index, feature] = -9999\n",
    "\n",
    "#For Well 16/7-4\n",
    "index_to_remove = df_labeled_with_features_sampling.index[(df_labeled_with_features_sampling['WELL'] == '25/11-5') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 1900) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 1950)].tolist()\n",
    "for index in index_to_remove:\n",
    "    for feature in ['RHOB', 'DTC']:\n",
    "        df_labeled_with_features_sampling.loc[index, feature] = -9999\n",
    "\n",
    "#For Well 31/3-4\n",
    "index_to_remove = df_labeled_with_features_sampling.index[(df_labeled_with_features_sampling['WELL'] == '31/3-4') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2000) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2050)].tolist()\n",
    "for index in index_to_remove:\n",
    "    for feature in ['RHOB', 'DTC']:\n",
    "        df_labeled_with_features_sampling.loc[index, feature] = -9999\n",
    "####################################################################################################################################\n",
    "\n",
    "\n",
    "#For Well 31/2-10\n",
    "index_to_remove = df_labeled_with_features_sampling.index[(df_labeled_with_features_sampling['WELL'] == '31/2-10') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 1650) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 1700)].tolist()\n",
    "for index in index_to_remove:\n",
    "    for feature in ['NPHI', 'RHOB']:\n",
    "        df_labeled_with_features_sampling.loc[index, feature] = -9999\n",
    "\n",
    "\n",
    "#For Well 35/11-11\n",
    "index_to_remove = df_labeled_with_features_sampling.index[(df_labeled_with_features_sampling['WELL'] == '35/11-11') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 3000) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 3050)].tolist()\n",
    "for index in index_to_remove:\n",
    "    for feature in ['NPHI', 'RHOB']:\n",
    "        df_labeled_with_features_sampling.loc[index, feature] = -9999\n",
    "\n",
    "\n",
    "#For Well 31/6-5\n",
    "index_to_remove = df_labeled_with_features_sampling.index[(df_labeled_with_features_sampling['WELL'] == '31/6-5') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 1950) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2000)].tolist()\n",
    "for index in index_to_remove:\n",
    "    for feature in ['NPHI', 'RHOB']:\n",
    "        df_labeled_with_features_sampling.loc[index, feature] = -9999\n",
    "\n",
    "####################################################################################################################################\n",
    "\n",
    "#For Well 16/7-6\n",
    "index_to_remove = df_labeled_with_features_sampling.index[(df_labeled_with_features_sampling['WELL'] == '16/7-6') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2350) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2400)].tolist()\n",
    "for index in index_to_remove:\n",
    "    for feature in ['DTC', 'NPHI']:\n",
    "        df_labeled_with_features_sampling.loc[index, feature] = -9999\n",
    "\n",
    "\n",
    "# For Well 31-2-21 S\n",
    "index_to_remove = df_labeled_with_features_sampling.index[(df_labeled_with_features_sampling['WELL'] == '31/2-21 S') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2800) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2850)].tolist()\n",
    "for index in index_to_remove:\n",
    "    for feature in ['DTC', 'NPHI']:\n",
    "        df_labeled_with_features_sampling.loc[index, feature] = -9999\n",
    "\n",
    "\n",
    "# For Well 34/7-13\n",
    "index_to_remove = df_labeled_with_features_sampling.index[(df_labeled_with_features_sampling['WELL'] == '34/7-13') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2800) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2850)].tolist()\n",
    "for index in index_to_remove:\n",
    "    for feature in ['DTC', 'NPHI']:\n",
    "        df_labeled_with_features_sampling.loc[index, feature] = -9999\n",
    "\n",
    "####################################################################################################################################        \n",
    "        \n",
    "        \n",
    "## Save file for later\n",
    "# Test Data file\n",
    "df_labeled_with_features_sampling.to_csv('df_labeled_with_features_sampling_with_multiple_logs_removed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell extracts data points from the previous cell to make the \"test data matrix\" for each of the three features\n",
    "\n",
    "#RHOB\n",
    "df_orig_1 = df_labeled_with_features_original[(df_labeled_with_features_original['WELL'] == '35/11-10') & (df_labeled_with_features_original['DEPTH_TVD'] > 2600) & (df_labeled_with_features_original['DEPTH_TVD'] < 2650)]\n",
    "df_orig_2 = df_labeled_with_features_original[(df_labeled_with_features_original['WELL'] == '25/11-5') & (df_labeled_with_features_original['DEPTH_TVD'] > 1900) & (df_labeled_with_features_original['DEPTH_TVD'] < 1950)]\n",
    "df_orig_3 = df_labeled_with_features_original[(df_labeled_with_features_original['WELL'] == '31/3-4') & (df_labeled_with_features_original['DEPTH_TVD'] > 2000) & (df_labeled_with_features_original['DEPTH_TVD'] < 2050)]\n",
    "\n",
    "#NPHI\n",
    "df_orig_4 = df_labeled_with_features_original[(df_labeled_with_features_original['WELL'] == '31/2-10') & (df_labeled_with_features_original['DEPTH_TVD'] > 1650) & (df_labeled_with_features_original['DEPTH_TVD'] < 1700)]\n",
    "df_orig_5 = df_labeled_with_features_original[(df_labeled_with_features_original['WELL'] == '35/11-11') & (df_labeled_with_features_original['DEPTH_TVD'] > 3000) & (df_labeled_with_features_original['DEPTH_TVD'] < 3050)]\n",
    "df_orig_6 = df_labeled_with_features_original[(df_labeled_with_features_original['WELL'] == '31/6-5') & (df_labeled_with_features_original['DEPTH_TVD'] > 1950) & (df_labeled_with_features_original['DEPTH_TVD'] < 2000)]\n",
    "\n",
    "#DTC\n",
    "df_orig_7 = df_labeled_with_features_original[(df_labeled_with_features_original['WELL'] == '31/2-21 S') & (df_labeled_with_features_original['DEPTH_TVD'] > 2800) & (df_labeled_with_features_original['DEPTH_TVD'] < 2850)]\n",
    "df_orig_8 = df_labeled_with_features_original[(df_labeled_with_features_original['WELL'] == '16/7-6') & (df_labeled_with_features_original['DEPTH_TVD'] > 2350) & (df_labeled_with_features_original['DEPTH_TVD'] < 2400)]\n",
    "df_orig_9 = df_labeled_with_features_original[(df_labeled_with_features_original['WELL'] == '34/7-13') & (df_labeled_with_features_original['DEPTH_TVD'] > 2800) & (df_labeled_with_features_original['DEPTH_TVD'] < 2850)]\n",
    "\n",
    "df_original_test = pd.concat([df_orig_1, df_orig_2, df_orig_3, df_orig_4, df_orig_5, df_orig_6, df_orig_7, df_orig_8, df_orig_9])\n",
    "\n",
    "# Save File for Later\n",
    "df_original_test.to_csv('df_original_test.csv') # Test Data matrix (Original Data)\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "#RHOB\n",
    "df_sample_1 = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['WELL'] == '35/11-10') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2600) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2650)]\n",
    "df_sample_2 = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['WELL'] == '25/11-5') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 1900) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 1950)]\n",
    "df_sample_3 = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['WELL'] == '31/3-4') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2000) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2050)]\n",
    "\n",
    "#NPHI\n",
    "df_sample_4 = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['WELL'] == '31/2-10') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 1650) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 1700)]\n",
    "df_sample_5 = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['WELL'] == '35/11-11') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 3000) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 3050)]\n",
    "df_sample_6 = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['WELL'] == '31/6-5') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 1950) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2000)]\n",
    "\n",
    "#DTC\n",
    "df_sample_7 = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['WELL'] == '31/2-21 S') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2800) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2850)]\n",
    "df_sample_8 = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['WELL'] == '16/7-6') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2350) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2400)]\n",
    "df_sample_9 = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['WELL'] == '34/7-13') & (df_labeled_with_features_sampling['DEPTH_TVD'] > 2800) & (df_labeled_with_features_sampling['DEPTH_TVD'] < 2850)]\n",
    "\n",
    "df_sample_test = pd.concat([df_sample_1, df_sample_2, df_sample_3, df_sample_4, df_sample_5, df_sample_6, df_sample_7, df_sample_8, df_sample_9])\n",
    "\n",
    "# Save File for Later\n",
    "df_sample_test.to_csv('df_sample_test_with_multiple_logs_removed.csv') # Test Data matrix with missing intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell creates the \"Neighbor Data Matrix\" which utilizes the remaining data excluding data points that is used for prediction\n",
    "\n",
    "# From this cell, if the following files created from previous cells are imported, run the ipynb file starting from this cell.\n",
    "df_labeled_with_features_sampling = pd.read_csv('df_labeled_with_features_sampling_with_multiple_logs_removed.csv')\n",
    "df_sample_test = pd.read_csv('df_sample_test_with_multiple_logs_removed.csv')\n",
    "\n",
    "\n",
    "RHOB_unknown = df_sample_test[(df_sample_test['RHOB']==-9999) & (df_sample_test['DTC']==-9999)]\n",
    "\n",
    "NPHI_unknown = df_sample_test[(df_sample_test['NPHI']==-9999) & (df_sample_test['RHOB']==-9999)]\n",
    "\n",
    "DTC_unknown = df_sample_test[(df_sample_test['DTC']==-9999) & (df_sample_test['NPHI']==-9999)]\n",
    "\n",
    "\n",
    "features_df_dict ={    \n",
    "    \n",
    "    'RHOB_unknown': RHOB_unknown,\n",
    "    \n",
    "    'NPHI_unknown': NPHI_unknown,\n",
    "    \n",
    "    'DTC_unknown': DTC_unknown\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Neighbor Data Matrix\n",
    "features_known_df = df_labeled_with_features_sampling[(df_labeled_with_features_sampling['GR'] != -9999) & (df_labeled_with_features_sampling['RHOB'] != -9999) & (df_labeled_with_features_sampling['NPHI'] != -9999) & (df_labeled_with_features_sampling['DTC'] != -9999)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collabortive Filtering Algorithm on Well Log Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: The following cell may take a very long time depending on your specifications.\n",
    "\n",
    "Result files already been created so you may utilize those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Collabortive Filtering##\n",
    "\n",
    "# Cosine Similarity Function\n",
    "def new_cosine(u_df, v_df):\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    compare_df = pd.concat([u_df, v_df])\n",
    "    compare_df_reset = compare_df.reset_index(drop=True)\n",
    "    compare_df_drop = compare_df_reset.replace(-9999, 0)\n",
    "    compare_df_drop = compare_df_drop.drop(columns=['Lithology_Type']) # Lithology Information is dropped as our research assumes lithology information is unknown\n",
    "\n",
    "    target_array = np.array(compare_df_drop.values)[[0]]\n",
    "    compare_arrays = np.array(compare_df_drop.values)[1:]\n",
    "\n",
    "    cosine_sim = cosine_similarity(target_array, compare_arrays)\n",
    "\n",
    "    return cosine_sim[0]\n",
    "\n",
    "\n",
    "# Collaborive FIltering Algorithm\n",
    "feature_list = ['RHOB', 'NPHI', 'DTC']\n",
    "cos_sim_compare_num_list = [2, 5, 10, 40, 90, 130, 170] # Number of Neighbors k (Change as will)\n",
    "for feature in feature_list:\n",
    "    for cos_sim_compare_num in cos_sim_compare_num_list:\n",
    "        features_known_df = features_known_df\n",
    "        single_feature_unknown_df = features_df_dict.get('%s_unknown' % feature)\n",
    "        features_known_df = features_known_df[['GR', 'RHOB', 'NPHI', 'DTC', 'Lithology_Type']]\n",
    "        single_feature_unknown_df = single_feature_unknown_df[['GR', 'RHOB', 'NPHI', 'DTC', 'Lithology_Type']]\n",
    "\n",
    "\n",
    "        for index in single_feature_unknown_df.index:\n",
    "            lith = single_feature_unknown_df.loc[index, 'Lithology_Type']\n",
    "            cosine_sim = new_cosine(single_feature_unknown_df.loc[[index]], features_known_df)\n",
    "            related_doc_indices = cosine_sim.argsort()\n",
    "\n",
    "            features_known_without_lith_df = features_known_df.drop(columns=['Lithology_Type'])\n",
    "\n",
    "            # Use Top k Values\n",
    "            if len(cosine_sim) < cos_sim_compare_num:\n",
    "                row_index_total = np.array(features_known_without_lith_df.index.tolist())[related_doc_indices[::-1]][0:].tolist()\n",
    "            else:\n",
    "                most_similar_row_index = np.array(features_known_without_lith_df.index.tolist())[related_doc_indices[::-1]][0:].tolist()\n",
    "                row_index_total = most_similar_row_index[0:cos_sim_compare_num]\n",
    "\n",
    "            if len(cosine_sim) < cos_sim_compare_num:\n",
    "                cosine_sim_total = cosine_sim\n",
    "            else:\n",
    "                cosine_sim_total = cosine_sim[related_doc_indices[::-1]][0:cos_sim_compare_num]\n",
    "\n",
    "            # Weighted Sum Method\n",
    "            features_known_without_lith_df_cs_total = features_known_without_lith_df.loc[row_index_total].reset_index(drop='True')\n",
    "            features_known_without_lith_df_cs_total['Cos_Sim'] = pd.Series(cosine_sim_total)\n",
    "            features_known_without_lith_df_cs_total['Weighted_Sum'] = features_known_without_lith_df_cs_total[feature] * features_known_without_lith_df_cs_total['Cos_Sim']\n",
    "\n",
    "            predicted_value = features_known_without_lith_df_cs_total['Weighted_Sum'].sum() / sum(cosine_sim_total)\n",
    "\n",
    "            lith_count_df = features_known_df.loc[row_index_total].reset_index(drop='True').Lithology_Type.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "\n",
    "\n",
    "            # Final\n",
    "            df_sample_test.loc[index, feature] = predicted_value\n",
    "        \n",
    "        # Predicted Test Data Matrix using k neighbors\n",
    "        df_sample_test.to_csv('df_sample_test_%s_%s_with_multiple_logs_removed.csv'%(feature, cos_sim_compare_num))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOaxWvx47Yg0QTn/AJDY+0f",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
